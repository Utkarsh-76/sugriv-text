import os
import json
from redis import Redis
import numpy as np
from dotenv import load_dotenv
from redis.commands.search.query import Query
from redis.commands.search.field import TagField, VectorField
from redis.commands.search.indexDefinition import IndexDefinition, IndexType
from redis.commands.search.query import Query
from  src.utils.logger import logging as logger
from src.llm.embeddings.text_embeddings import TextEmbeddings

load_dotenv()

REDIS_URL = str(os.getenv("REDIS_URL"))
INDEX_NAME = "prompt_completion"                  # Vector Index Name
DOC_PREFIX = "doc:"                               # RediSearch Key Prefix for the Index
D_MODEL = int(os.getenv("D_MODEL"))


class RedisDB():

    def __init__(self) -> None:
        ''' the redis sdk creates a standard way to cache prompts and completions '''
        try:
            logger.info(f'inititalizing the redis connection at { REDIS_URL }')
            self.client = Redis.from_url(REDIS_URL)
            self.prompt_completion_index = self.create_index(D_MODEL)
            logger.info('inititating the redis pipeline')
        except RuntimeError as e:
            logger.error('can not cache the completion from the llm')
            logger.error(e)

        self.pipe = self.client.pipeline()
        self.text_embedding_model = TextEmbeddings().embed_model

    def put(self,prompt,completion,score):
        ''' 
            adds the embedding of the prompt sinto the the db with the prompt as the key and the value having the following
              1. prompt - the prompt input to the llm
              2. completion - the completion from the llm
              3. score - the score generated by the llm 
              4. prompt_embedding - the embedding for the last token of the prompt
        '''
        try:
            objects = {"prompt": prompt, "completion": completion,"score":score}
            logger.info(f"adding object {json.dumps(objects)} to redis db" )
            key = f"doc:{objects['prompt']}"
            objects["prompt_embedding"] =  np.array(self.text_embedding_model.get_text_embedding(prompt)).astype(dtype=np.float32).tobytes()
            self.pipe.hset(key, mapping=objects)
            res = self.pipe.execute()
            return res
        except RuntimeError as e :
            logger.error('can not put the completion from the llm ')
            logger.error(e)

    def get_top_k(self,prompt,top_k):
        ''' 
        get the prompt and the query for the top K similar items based on 
        the embedding of the last token then sort the result by cosine distance
        to get the top k similar prompt embeddings 
        '''
        try:
            query = Query(f'*=>[KNN {top_k} @prompt_embedding $vec]=>{{$yield_distance_as: dist}}').sort_by(f'dist').return_fields("prompt","completion","score","dist").dialect(2)
            query_params = {
                "vec":  np.array(self.text_embedding_model.get_text_embedding(prompt)).astype(dtype=np.float32).tobytes()
            }
            result = self.client.ft(INDEX_NAME).search(query, query_params)
            return result.docs
        except RuntimeError as e :
            logger.error('can not get k nearest from cache getting from llm')
            logger.error(e)
    
    def create_index(self,vector_dimensions: int):
        logger.info(f"creating index {INDEX_NAME} in redis")
        try:
            self.client.ft(INDEX_NAME).info()
            logger.error("Index already exists!")
        except:
            schema = (
                TagField("prompt"),  
                TagField("completion"),    
                TagField("score"),                     # Tag Field Name
                VectorField("prompt_embedding",        # Vector Field Name
                    "FLAT", {                          # Vector Index Type: FLAT or HNSW
                        "TYPE": "FLOAT32",             # FLOAT32 or FLOAT64
                        "DIM": vector_dimensions,      # Number of Vector Dimensions
                        "DISTANCE_METRIC": "COSINE",   # Vector Search Distance Metric
                    }
                ),
            )

            definition = IndexDefinition(prefix=[DOC_PREFIX], index_type=IndexType.HASH)
            index = self.client.ft(INDEX_NAME).create_index(fields=schema, definition=definition)
            logger.info(f"created index {INDEX_NAME} in redis")
            return index